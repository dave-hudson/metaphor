Goal:
    Create a test runner for the metaphor compiler

    An implementation of an earlier version of this has already been implemented:

    Code: src/testrun/testrun.py

    Story:
        Core functionality.

        As: an engineer working on the metaphor compiler,
        I: want to run automated tests to check the compiler is working correctly,
        So: my users have confidence in the output form the compiler.

        Require: The test runner will run a series of tests against the metaphor compiler.

        Require:
            The test runner can execute positive and negative tests.  Positive tests are expected to generate good output,
            while negative tests are expected to produce errors.

        Require:
            Each test will be started in a new process.  The test runner will track the exit status of each test and use it
            to determine if tests have passed or failed.

        Require:
            When a test starts, display a message to the console with the test command being executed.

            Example:
                Given: the test runner starts a test
                When: the test starts
                Then: print a console message: "Start <test command>", where <test command> is the command that will be executed.

        Require: The test runner must capture the console output of each test.

        Require:
            A test process must exit with a zero status for a passing positive test, or a non-zero status for a passing
            negative test.

        Require:
            If the test command exits with a zero status for a positive test, or a non-zero status for a negative test, then
            check if there is an expected results file.  If there is no expected results file then ignore any console output.  If
            a test has an expected result then the console output from the test must be compared with output in the
            expected result file.  If these do not match then the test has failed.  If they do match then the test has passed.
            The compare operation is a simple character by character string compare.

        Require:
            When a test passes or fails, the details will be recorded and a summary displayed once all tests have been processed.

        Require:
            Some tests may fail because they take too long.  If this happens, then the test should be terminated
            by sending a KILL signal to the process that has taken too long.  By default the timeout is set at 5000 ms.
            Any test that times out will be recorded as a fail.

        Require:
            When a test completes, display a message to the console indicating if it passed or failed.

            Example:
                Given: a test completes,
                When: the test passes,
                Then: print a console message: "PASS: <test command>", where <test command> is the test command that was executed.

            Example:
                Given: a test completes,
                When: the test fails,
                Then: print a console message: "FAIL: <test command>", where <test command> is the test command that was executed.

    Story:
        Performance.

        As: an engineer working on the metaphor compiler,
        I: would like my tests to execute as quickly as possible,
        So: I wish them to execute in parallel.

        Require:
            The test runner must be able to execute tests in parallel, up to some maximum limit.  The limit will default to
            the number of CPU cores on the system running the tests.  Tests have no dependencies on each other and can thus
            be run in any order.

    Story:
        Test configuration.

        As: an engineer working on the metaphor compiler,
        I: would like my tests to be configured via a single test configuration file,
        So: I can easily configure tests and test settings.

        Require: The set of tests wil be defined by a JSON test configuration file.

        Require: For each test there will be a series of JSON key/value pairs.

        Require:
            Each test has a set of mandatory or optional key/value pairs

            Require: Mandatory key/value: "command".  This is the test command that will be executed to run the test.

            Require:
                Mandatory key/value: "type".  Indicates if this is a positive or negative test.  Values are either
                "positive" or "negative".

            Require:
                Optional key/value: "expected".  If this key/value pair exists, it specifies a file that contains expected
                results from the test.

            Require:
                Optional key/value: "timeout".  If this key/value pair exists, it sets the timeout, in ms, for this test,
                and overrides the default timeout.

            Example:
                The following is an example of two tests configured in a JSON test configuration file:

                ```json
                [
                    {
                        "command": "build/metaphorc test/test1.m6r",
                        "type": "positive",
                        "timeout": 1000
                    },
                    {
                        "command": "build/metaphorc test/test2.m6r",
                        "type": "negative"
                    }
                ]
                ```

    Story:
        Tool invocation.

        As: an engineer working on the metaphor compiler,
        I: would like my tests be be started by executing the test runner from the command line,
        So: I can quickly run and tests and configure test behaviour.

        Require: The test runner can be run from the command line with appropriate command line parameters.

        Require: The default command line parameter is the name of the file with the test configurations.

        Require: The number of parallel tests that can be run can be specified with the "--parallel-tests" command line parameter.

        Require:
            If the user provides invalid parameters (any that are not specified in this requirement), or passes the "-h"
            or "--help" command line parameters, then the test runner should output the usage options for the test runner
            to the console and exit with an error status.

        Require:
            When the test runner completes it will exit with a status code of 0 if all tests have passed, or with a status code
            of 1 if any of the tests failed.

    Story:
        Implementation.

        As: an engineer working on the metaphor compiler,
        I: want my test runner to be easy to use and understand,
        So: I can maintain and enhance it over time.

        Require: The test runner will be built with the latest version of python 3.

        Require: Code should be indented by 4 spaces.

        Require:
            All filesystem operations should be checked to see if they have completed successfully.  If any fail then these
            should be handled as failures against the test that was being run.

        Require:
            The config file must be checked to ensure there are no keys that are not defined in this requirement.  If any
            are found then the test runner should emit a console error with the line number and exit with an error status.

        Require:
            The test runner code should be commented to make it easy to understand how it works.  Comments should include
            doc strings for all functions, and comments to explain any complex logic.

        Require:
            Do not use elif or else statements if the preceding statement returns or calls sys.exit
